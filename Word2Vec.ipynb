{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install underthesea"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IciBJxiMYDB_",
        "outputId": "4c35b41a-85ce-420b-e480-c53e96d62c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting underthesea\n",
            "  Downloading underthesea-6.2.0-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.3)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.27.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0)\n",
            "Collecting underthesea-core==1.0.0 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.0-cp310-cp310-manylinux2010_x86_64.whl (599 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.6/599.6 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2022.10.31)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.9 underthesea-6.2.0 underthesea-core-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "from underthesea import word_tokenize\n",
        "\n",
        "def softmax(x):\n",
        "\t\"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "\te_x = np.exp(x - np.max(x))\n",
        "\treturn e_x / e_x.sum()\n",
        "# tạo lớp w2v\n",
        "class word2vec(object):\n",
        "\tdef __init__(self):\n",
        "\t\tself.N = 100\n",
        "\t\tself.X_train = []\n",
        "\t\tself.y_train = []\n",
        "\t\tself.window_size = 4\n",
        "\t\tself.alpha = 0.001\n",
        "\t\tself.words = []\n",
        "\t\tself.word_index = {}\n",
        "\n",
        "\tdef initialize(self,V,data):\n",
        "\t\tself.V = V\n",
        "\t\tself.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
        "\t\tself.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
        "\t\t\n",
        "\t\tself.words = data\n",
        "\t\tfor i in range(len(data)):\n",
        "\t\t\tself.word_index[data[i]] = i\n",
        "\n",
        "\tdef feed_forward(self,X):\n",
        "\t\tself.h = np.dot(self.W.T,X).reshape(self.N,1)\n",
        "\t\tself.u = np.dot(self.W1.T,self.h)\n",
        "\t\t#print(self.u)\n",
        "\t\tself.y = softmax(self.u)\n",
        "\t\treturn self.y\n",
        "\t# cập nhật trọng số cho từ\n",
        "\tdef backpropagate(self,x,t):\n",
        "\t\te = self.y - np.asarray(t).reshape(self.V,1)\n",
        "\t\t# e.shape is V x 1\n",
        "\t\tdLdW1 = np.dot(self.h,e.T)\n",
        "\t\tX = np.array(x).reshape(self.V,1)\n",
        "\t\tdLdW = np.dot(X, np.dot(self.W1,e).T)\n",
        "\t\tself.W1 = self.W1 - self.alpha*dLdW1\n",
        "\t\tself.W = self.W - self.alpha*dLdW\n",
        "\t\t# train model\n",
        "\tdef train(self,epochs):\n",
        "\t\tfor x in range(1,epochs):\t\n",
        "\t\t\tself.loss = 0\n",
        "\t\t\tfor j in range(len(self.X_train)):\n",
        "\t\t\t\tself.feed_forward(self.X_train[j])\n",
        "\t\t\t\tself.backpropagate(self.X_train[j],self.y_train[j])\n",
        "\t\t\t\tC = 0\n",
        "\t\t\t\tfor m in range(self.V):\n",
        "\t\t\t\t\tif(self.y_train[j][m]):\n",
        "\t\t\t\t\t\tself.loss += -1*self.u[m][0]\n",
        "\t\t\t\t\t\tC += 1\n",
        "\t\t\t\tself.loss += C*np.log(np.sum(np.exp(self.u)))\n",
        "\t\t\tprint(\"epoch \",x, \" loss = \",self.loss)\n",
        "\t\t\tself.alpha *= 1/( (1+self.alpha*x) )\n",
        "\t\t\t# dự đoán model\n",
        "\tdef predict(self,word,number_of_predictions):\n",
        "\t\tif word in self.words:\n",
        "\t\t\tindex = self.word_index[word]\n",
        "\t\t\tX = [0 for i in range(self.V)]\n",
        "\t\t\tX[index] = 1\n",
        "\t\t\tprediction = self.feed_forward(X)\n",
        "\t\t\toutput = {}\n",
        "\t\t\tfor i in range(self.V):\n",
        "\t\t\t\toutput[prediction[i][0]] = i\n",
        "\t\t\t\n",
        "\t\t\ttop_context_words = []\n",
        "\t\t\tfor k in sorted(output,reverse=True):\n",
        "\t\t\t\ttop_context_words.append(self.words[output[k]])\n",
        "\t\t\t\tif(len(top_context_words)>=number_of_predictions):\n",
        "\t\t\t\t\tbreak\n",
        "\t\n",
        "\t\t\treturn top_context_words\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Word not found in dictionary\")\n",
        "  # xử lý dữ liệu\n",
        "def preprocessing(corpus):\n",
        "\ttraining_data = []\n",
        "\tsentences = corpus.split(\".\")\n",
        "\tfor i in range(len(sentences)):\n",
        "\t\t# Tách từ\n",
        "\t\tsentence= word_tokenize(sentences[i])\n",
        "\t\tx = [word.strip(string.punctuation) for word in sentence]\n",
        "\t\tx = [word.lower() for word in x]\n",
        "\t\ttraining_data.append(x)\n",
        "\treturn training_data\n",
        "\t\n",
        "\n",
        "def prepare_data_for_training(sentences,w2v):\n",
        "\tdata = {}\n",
        "\tfor sentence in sentences:\n",
        "\t\tfor word in sentence:\n",
        "\t\t\tif word not in data:\n",
        "\t\t\t\tdata[word] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tdata[word] += 1\n",
        "\tV = len(data)\n",
        "\tdata = sorted(list(data.keys()))\n",
        "\tvocab = {}\n",
        "\tfor i in range(len(data)):\n",
        "\t\tvocab[data[i]] = i\n",
        "\t\n",
        "\t#for i in range(len(words)):\n",
        "\tfor sentence in sentences:\n",
        "\t\tfor i in range(len(sentence)):\n",
        "\t\t\tcenter_word = [0 for x in range(V)]\n",
        "\t\t\tcenter_word[vocab[sentence[i]]] = 1\n",
        "\t\t\tcontext = [0 for x in range(V)]\n",
        "\t\t\t\n",
        "\t\t\tfor j in range(i-w2v.window_size,i+w2v.window_size):\n",
        "\t\t\t\tif i!=j and j>=0 and j<len(sentence):\n",
        "\t\t\t\t\tcontext[vocab[sentence[j]]] += 1\n",
        "\t\t\tw2v.X_train.append(center_word)\n",
        "\t\t\tw2v.y_train.append(context)\n",
        "\tw2v.initialize(V,data)\n",
        "\n",
        "\treturn w2v.X_train,w2v.y_train\n"
      ],
      "metadata": {
        "id": "Qi53nIwOgHru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus='Sông Hàn là một nhánh sông lớn đổ ra biển, có lẽ đây đã là dòng chảy cuối trên hành trình đổ vào biển Đông nên dường như dòng sông chảy chậm lại, lững lờ và uốn lượn quanh co. Sông Hàn rộng dài, đứng ở bên này sông nhìn sang bên kia sông những tòa nhà dường như trở nên bé nhỏ. Đi bộ buổi sáng ven sông Hàn để đón chào bình minh là khoảnh khắc tuyệt vời nhất, khi trời vừa hừng đông, từng gợn sóng lăn tăn bắt đầu nổi lên như sự thức giấc của sông Hàn, mặt nước cũng ửng hồng như còn đang ngái ngủ. Chờ đến khi mặt trời lên, dòng sông như bừng tỉnh, ngàn tia nắng chiếu xuống mặt nước lung linh lấp lánh, và thế là sông Hàn sẽ mặc chiếc áo óng ánh đó suốt cả một ngày dài. Cho đến tận chiều tối, mặt trời chưa kịp lặn thì thành phố đã lên đèn, sông Hàn trở nên sặc sỡ hơn bao giờ hết, chiếc cầu quay với đèn xanh, đỏ, vàng in bóng xuống dòng sông thật lung linh huyền ảo. Vì vẻ đẹp về đêm và khung cảnh lãng mạn của sông Hàn mà ở đây có thêm cây cầu tình yêu, một nơi lý tưởng để tâm sự, bày tỏ yêu thương. Sông Hàn mang một vẻ đẹp thơ mộng, yêu kiều và cũng rất bận rộn, hiếm thấy có con sông nào giữa lòng thành phố lại nhộn nhịp tàu thuyền chở hàng qua lại nhiều như sông Hàn. Đây dường như là một dòng sông không ngủ, từ sáng sớm đến đêm muộn dòng sông dập dìu sóng nước đã tiễn đưa và chào đón biết bao lượt tàu thuyền qua lại, chính vì sự bận rộn của hoạt động vận tải trên sông nên mới có chiếc cầu quay sông Hàn, vào thời điểm ban đêm hoặc lúc có thuyền to cần đi qua cầu, chiếc cầu sẽ quay dọc theo dòng sông để thuyền có thể đi qua, sau đó lại nối lại để hai bên bờ sông có thể đi lại. Sông Hàn đẹp nhất về đêm bởi khi đó nó như một dòng sông chảy giữa ánh sáng, càng tuyệt đẹp hơn nữa là những đêm pháo hoa trên sông Hàn, khi ấy dòng sông Hàn dường như đang thăng hoa, biến mình trở thành một dòng sông ngàn hoa nở rộ đầy sắc màu rực rỡ. Dòng sông Hàn nằm giữa thành phố Đà Nẵng, nơi mà bất kì du khách du lịch nào cũng muốn ghé thăm một lần. Quanh năm, dù là khi nào sông cũng đầy ăm ắp. Nước sông cứ cuồn cuộn, đong đầy như tình cảm nhiệt thành của người dân vùng đất này. Nước sông Hàn không có màu trong vắt như nước mùa thu, hay xanh biếc như bầu trời mùa hạ. Mà lúc nào cũng có màu xanh sẫm huyền bí. Màu xanh ấy, một phần được ánh từ bầu trời bao la phía trên cao, một phần chính bởi vì lòng sông sâu và rộng. Mặt sông Hàn vốn là phẳng lặng. Trông như một tấm gương dành riêng cho những người khổng lồ. Thế nhưng, thật chẳng mấy khi được xem một sông Hàn tĩnh lặng, bởi nơi đây lúc nào cũng tấp nập những chiếc tàu thuyền ngược xuôi với đầy những hàng hóa. Chúng rẽ ra từng cột sóng lớn, bọt nước vùng lên trắng xóa, như vẽ lên chiếc đuôi cá cho con thuyền. Điểm nhấn đẹp nhất của sông Hàn chính là cầu sông Hàn - một cái tên thật mộc mạc và chân chất. Chiếc cầu được thiết kế hình con rồng uốn lượn rất kì công, được xem như một trong những niềm tự hào của người dân Đà Nẵng nói riêng và Việt Nam nói chung. Chính chiếc cầu đã làm tăng thêm vẻ đẹp của con sông. Nhưng đồng thời, đó cũng là một điểm tựa để mọi người có thể quan sát một cách toàn vẹn vẻ đẹp tuyệt vời của con sông ấy. Đặc biệt là vào những buổi đêm. Khi thành phố lên đèn, khi chiếc cầu sáng rực lên với những ánh sáng lấp lánh. Chúng hắt xuống dòng sông, để mặt sông ánh lên muôn vàn màu sắc kì diệu, lung linh như thể đó là dòng sông dẫn đến xứ sở thần tiên.'\n",
        "epochs = 5\n",
        "training_data = preprocessing(corpus)\n",
        "w2v = word2vec()\n",
        "\n",
        "prepare_data_for_training(training_data,w2v)\n",
        "w2v.train(epochs)\n"
      ],
      "metadata": {
        "id": "EwAbo_SnU3-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e886483-47ec-45af-a9e8-9bd848abcbb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch  1  loss =  31998.15307983864\n",
            "epoch  2  loss =  31633.64810169326\n",
            "epoch  3  loss =  31290.53553830176\n",
            "epoch  4  loss =  30967.538863996546\n",
            "['lên đèn', 'đã', 'dân', 'muốn', 'tăng', 'phía', 'tàu thuyền', 'đó']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # tính độ tương đồng của các từ\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(word1, word2, weights, word_index):\n",
        "    index1 = word_index[word1]\n",
        "    index2 = word_index[word2]\n",
        "    \n",
        "    vector1 = weights[index1]\n",
        "    vector2 = weights[index2]\n",
        "    \n",
        "    dot_product = np.dot(vector1, vector2)\n",
        "    norm1 = np.linalg.norm(vector1)\n",
        "    norm2 = np.linalg.norm(vector2)\n",
        "    \n",
        "    similarity = dot_product / (norm1 * norm2)\n",
        "    \n",
        "    return similarity\n",
        "\n",
        "# Sử dụng ví dụ từ vựng và trọng số từ câu hỏi trướ\n",
        "word1='màu sắc'\n",
        "word2='lung linh'\n",
        "similarity = cosine_similarity(word1, word2, w2v.W, w2v.word_index)\n",
        "print(f\"Độ tương đồng cosine giữa '{word1}' và '{word2}': {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VULNxVoJ2ySA",
        "outputId": "be4deb9f-b682-4427-860f-e22a2332afb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Độ tương đồng cosine giữa 'màu sắc' và 'lung linh': 0.01883271633242749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lấy embedding từng từ\n",
        "weights = w2v.W\n",
        "words=[]\n",
        "vectors=[]\n",
        "for word, index in w2v.word_index.items():\n",
        "    words.append(word)\n",
        "    vectors.append(weights[index])\n"
      ],
      "metadata": {
        "id": "btrw5yBLhbIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "words=pd.DataFrame(words)\n",
        "vectors=pd.DataFrame(vectors)\n"
      ],
      "metadata": {
        "id": "o0DVKhqlsi2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=words.iloc[3:]\n",
        "vectors=vectors.iloc[3:]"
      ],
      "metadata": {
        "id": "V5KLMEfpbUUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words.to_csv('words.csv', sep='\\t', index=False, header=False)\n",
        "vectors.to_csv('vectors.csv', sep='\\t', index=False, header=False)"
      ],
      "metadata": {
        "id": "mOWx0uTaLMpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZI-UFDut4FL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}